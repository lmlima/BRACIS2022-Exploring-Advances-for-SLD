#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Author: Andr√© Pacheco
E-mail: pacheco.comp@gmail.com

This file implements the class metrics to be used in the evaluation phase

If you find any bug or have some suggestion, please, email me.
"""

import os
import torch
import numpy as np
from .utils import classification_metrics as cmet
import pandas as pd
import matplotlib.pyplot as plt


class Metrics:

    def __init__(self, metrics_names, class_names=None, options=None):
        """
        Construction method. You must inform the metrics you'd like to compute. Optionally, you may set some options 
        and the classes names. Each parameter is better described bellow:
        
        :param metrics_name (list, tuple or string): it's the variable that receives the metrics you'd like to compute.
        The following metrics are available: "accuracy", "topk_accuracy", "balanced_accuracy", "conf_matrix", 
        "plot_conf_matrix", "precision_recall_report", and "auc_and_roc_curve". To understand them, please, go to 
        jedy.utils.classification_metrics.py. You should pass one or more of these metrics in a list or a tuple, 
        for ex: m = ["accuracy", "conf_matrix"]. If you'd like to compute all of them, just set it as 'all', i.e., 
        m = 'all'
        
        Important: if you'd like to compute either 'plot_conf_matrix' or 'auc_and_roc_curve', you must inform the
        class_names. If not, you'll get an exception. The remaining metrics, except 'accuracy', also use the class_name,
        however, it's not demanded for them
        
        :param class_names (list, tuple): a list or tuple containing the classes names in the same order you use in the
        label. For ex: ['C1', 'C2']

        :param options (dict): this is a dict containing some options to compute the metrics. The following options are
        available:
        - For all:
            - save_all_path: a string with the path to save all metrics and images. In this case, the conf matrix will be
            called by conf_mat.png and the roc curve as roc_curve.png

        - For 'topk_accuracy':
            - 'topk' (int): if you'd like to compute the top k accuracy, you should inform the top k. If you don't,
            the default value is 2

        - For 'conf_matrix': 
            - 'normalize_conf_matrix' (bool): inform if you'd like to normalize the confusion matrix
        
        - For 'plot_conf_matrix':
            - 'save_path_conf_matrix' (string): the complete file path if you'd like to save instead show the plot
            - 'normalize_conf_matrix' (bool): inform if you'd like to normalize the confusion matrix
            - 'title_conf_matrix' (string): the plot's title
            
        - For 'auc_and_roc_curve':
            - 'save_path_roc_curve' (string): the complete file path if you'd like to save instead show the plot 
            - 'class_to_compute_roc_curve' (string): if you'd like to compute just one class instead all, you can set it
            here.

        - For 'save_scores':
            - 'save_path_scores' (string): the complete file path if to save the scores
            - 'pred_name_scores' (string): the names of the predictions .csv. If it's not informed, it's going to be
            predictions.csv
            
        For more information about the options, please, refers to jedy.utils.classification_metrics.py
         
        """
        self.metrics_names = metrics_names
        self.metrics_values = dict()        
        self.options = options
        
        self.pred_scores = None
        self.label_scores = None
        self.img_names = None
        
        self.class_names = class_names
        self.topk = None


    def compute_metrics (self):
        """
        This method computes all metrics defined in metrics_name.
        :return: it saves in self.metric_values all computed metrics
        """

        save_all_path = None
        # Checking if save_all is informed
        if self.options is not None:
            if "save_all_path" in self.options.keys():
                # Checking if the folder doesn't exist. If True, we must create it.
                if not os.path.isdir(self.options["save_all_path"]):
                    os.mkdir(self.options["save_all_path"])
                save_all_path = self.options["save_all_path"]

        if self.metrics_names is None:
            return None
        
        if self.metrics_names == "all":
            self.metrics_names = ["accuracy", "topk_accuracy", "balanced_accuracy",  "conf_matrix", "plot_conf_matrix",
                                  "precision_recall_report", "auc_and_roc_curve", "auc"]
        
        
        for mets in self.metrics_names:
            if mets == "accuracy":
                self.metrics_values["accuracy"] = cmet.accuracy(self.label_scores, self.pred_scores)
                
            elif mets == "balanced_accuracy":
                self.metrics_values["balanced_accuracy"] = cmet.balanced_accuracy(self.label_scores, self.pred_scores)

            elif mets == "topk_accuracy":

                # Checking if the class names are defined
                self.topk = 2
                if self.options is not None:
                    if "topk" in self.options.keys():
                        self.topk = self.options["topk"]

                self.metrics_values["topk_accuracy"] = cmet.topk_accuracy(self.label_scores, self.pred_scores, self.topk)
            
            elif mets == "conf_matrix":
                
                # Checking the options
                normalize = False
                if self.options is not None:
                    if "normalize_conf_matrix" in self.options.keys():
                        normalize = self.options["normalize_conf_matrix"]
                
                self.metrics_values["conf_matrix"] = cmet.conf_matrix(self.label_scores, self.pred_scores, normalize)
            elif mets == "plot_conf_matrix":
                
                # Checking if the class names are defined
                if self.class_names is None:
                    raise Exception ("You are trying to plot the confusion matrix without defining the classes name")
                
                # Checking the options
                save_path = None
                normalize = False
                title = "Confusion Matrix"   
                
                if self.options is not None:
                    if save_all_path is not None:
                        save_path = os.path.join(save_all_path, "conf_mat.png")
                    if "save_path_conf_matrix" in self.options.keys():
                        save_path = self.options["save_path_conf_matrix"]
                    if "normalize_conf_matrix" in self.options.keys():
                        normalize = self.options["normalize_conf_matrix"]
                    if "title_conf_matrix" in self.options.keys():
                        title = self.options["title_conf_matrix"]
                        
                if "conf_matrix" in self.metrics_values.keys():
                    cm = self.metrics_values["conf_matrix"]
                else:
                    cm = cmet.conf_matrix(self.label_scores, self.pred_scores, normalize)
                
                cmet.plot_conf_matrix(cm, self.class_names, normalize, save_path, title)
                
                
            elif mets == "precision_recall_report":

                self.metrics_values["precision_recall_report"] = cmet.precision_recall_report(self.label_scores,
                                                                                              self.pred_scores,
                                                                                              self.class_names)

            elif mets == 'auc':
                self.metrics_values["auc"] = cmet.roc_auc(self.label_scores, self.pred_scores)
                
            elif mets == "auc_and_roc_curve":
                
                # Checking if the class names are defined
                if self.class_names is None:
                    raise Exception ("You are trying to plot the confusion matrix without defining the classes name")

                # Checking the options
                save_path = None
                class_to_compute = "all"                

                if self.options is not None:
                    if save_all_path is not None:
                        save_path = os.path.join(save_all_path, "roc_curve.png")
                    if "save_path_roc_curve" in self.options.keys():
                        save_path = self.options["save_path_roc_curve"]
                    if "class_to_compute_roc_curve" in self.options.keys():
                        class_to_compute = self.options["class_to_compute_roc_curve"]

                self.metrics_values["auc_and_roc_curve"] = cmet.auc_and_roc_curve(self.label_scores, self.pred_scores,
                                                                                  self.class_names, class_to_compute, 
                                                                                  save_path)

            if save_all_path is not None:
                self.save_metrics(save_all_path)

    def print (self):
        """
        This method just prints the metrics on the screen
        """
        
        if self.metrics_names is None:
            print ("Since metrics name is None, there is no metric to print")
            
        else:        
            for met in self.metrics_values.keys():
                if met == "loss":
                    print ('- Loss: {:.3f}'.format(self.metrics_values[met]))
                elif met == "accuracy":
                    print ('- Accuracy: {:.3f}'.format(self.metrics_values[met]))
                elif met == "balanced_accuracy":
                    print ('- Balanced accuracy: {:.3f}'.format(self.metrics_values[met]))
                elif met == "topk_accuracy":
                    print('- Top {} accuracy: {:.3f}'.format(self.topk, self.metrics_values[met]))
                elif met == "auc":
                    print ('- AUC: {:.3f}'.format(self.metrics_values[met]))
                elif met == "conf_matrix":
                    print('- Confusion Matrix: \n{}'.format(self.metrics_values[met]))
                elif met == "precision_recall_report":
                    print('- Precision and Recall report: \n{}'.format(self.metrics_values[met]))
                elif met == "auc_and_roc_curve":
                    resp = self.metrics_values[met]
                    print('- AUC: \n{}'.format(resp[0]))

    def add_metric_value (self, value_name, value):
        """
        Adding a new value from a external source into the metrics
        :param value_name (string): the key for the dict
        :param value: the value to be saved in the self.metrics_values
        """
        self.metrics_values[value_name] = value


    def update_scores (self, label_batch, pred_batch, img_name_batch=None):
        """
        The evaluation is made using batchs. So, every batch we get just a piece of the prediction. This method
        concatenate all prediction and labels in order to compute the metrics
        :param pred (np.array): an array containing part of the predictions outputed by the model
        :param label (np.array): an array contaning the true labels
        """

        if self.label_scores is None and self.pred_scores is None:
            self.label_scores = label_batch
            self.pred_scores = pred_batch
            self.img_names = img_name_batch
        else:
            if pred_batch is not None:
                self.pred_scores = np.concatenate((self.pred_scores, pred_batch))
            if label_batch is not None:
                self.label_scores = np.concatenate((self.label_scores, label_batch))
            if img_name_batch is not None:
                self.img_names = np.concatenate((self.img_names, img_name_batch))


    def save_metrics (self, folder_path, name="metrics.txt"):
        """
        This method saves the computed metrics
        :param folder_path (string): the folder you'd like to save the metrics
        :param name (string): the file name. Default is metrics.txt
        """
        
        if self.metrics_names is None:
            print ("Since metrics name is None, there is no metric to save")
            
        else:        
            with open(os.path.join(folder_path, name), "w") as f:
    
                f.write("- METRICS REPORT -\n\n")
    
                for met in self.metrics_values.keys():
                    if met == "loss":
                        f.write('- Loss: {:.3f}\n'.format(self.metrics_values[met]))
                    elif met == "accuracy":
                        f.write('- Accuracy: {:.3f}\n'.format(self.metrics_values[met]))
                    elif met == "balanced_accuracy":
                        f.write('- Balanced accuracy: {:.3f}\n'.format(self.metrics_values[met]))
                    elif met == "topk_accuracy":
                        f.write('- Top {} accuracy: {:.3f}\n'.format(self.topk, self.metrics_values[met]))
                    elif met == "auc":
                        f.write('- AUC: {:.3f}\n'.format(self.metrics_values[met]))
                    elif met == "conf_matrix":
                        f.write('- Confusion Matrix: \n{}\n'.format(self.metrics_values[met]))
                    elif met == "precision_recall_report":
                        f.write('- Precision and Recall report: \n{}\n'.format(self.metrics_values[met]))
                    elif met == "auc_and_roc_curve":
                        resp = self.metrics_values[met]
                        f.write('- AUC:\n {}\n'.format(resp[0]))


    def save_scores (self, folder_path=None, pred_name="predictions.csv"):
        """
        This method saves the concatenated scores in the disk
        :param folder_path (string): the folder you'd like to save the scores
        :param pred_name (string): the predictions' score file name. Default is predictions.csv
        :param labels_name (string): the labels' score file name. Default is labels.csv
        """

        if folder_path is not None:
            # Checking if the folder doesn't exist. If True, we must create it.
            if not os.path.isdir(folder_path):
                os.mkdir(folder_path)
        elif self.options is not None:
            if "save_all_path" in self.options.keys():
                folder_path = self.options["save_all_path"]
            elif "save_path_scores" in self.options.keys():
                folder_path = self.options["save_path_scores"]
            else:
                raise ("The options doesnt have any folder to save the scores")

            if 'pred_name_scores' in self.options.keys():
                pred_name = self.options['pred_name_scores']
        else:
            raise ("You must set the path to save the score eithe in options or in folder_path parameter")


        # Getting the list of classications and predict labels
        if self.class_names is not None:
            if self.label_scores is not None:
                real_labels = [self.class_names[int(l)] for l in self.label_scores]
                real_labels = np.asarray(real_labels)
                real_labels = real_labels.reshape(real_labels.shape[0], 1)

            if self.img_names is not None:
                img_names = np.asarray(self.img_names)
                img_names = img_names.reshape(img_names.shape[0], 1)

            pred_labels = [self.class_names[ps.argmax()] for ps in self.pred_scores]
            pred_labels = np.asarray(pred_labels)
            pred_labels = pred_labels.reshape(pred_labels.shape[0], 1)
        else:
            raise ("You need to inform the class names to use this function")

        if self.img_names is not None and self.label_scores is not None:
            both_data = np.concatenate((img_names, real_labels, pred_labels, self.pred_scores), axis=1)
            cols = ['image', 'REAL', 'PRED', *self.class_names]
        elif self.img_names is None and self.label_scores is not None:
            both_data = np.concatenate((real_labels, pred_labels, self.pred_scores), axis=1)
            cols = ['REAL', 'PRED', *self.class_names]
        elif self.img_names is not None and self.label_scores is None:
            both_data = np.concatenate((img_names, pred_labels, self.pred_scores), axis=1)
            cols = ['image', 'PRED', *self.class_names]
        else:
            both_data = np.concatenate((real_labels, pred_labels, self.pred_scores), axis=1)
            cols = ['REAL', 'PRED', *self.class_names]

        df = pd.DataFrame(both_data, columns=cols)
        print ("Saving the scores in {}".format(folder_path))

        df.to_csv(os.path.join(folder_path, pred_name), index=False)



class TrainHistory:
    
    def __init__(self):
        self.val_loss = list()
        self.val_acc = list()
        self.train_loss = list()
        self.train_acc = list()
        
    
    def update (self, loss_train, loss_val, acc_train, acc_val):
        """
        This function appends a new value to the loss/train loss and acc. These values are stored by epoch
        :param loss_train: the train loss of the ith epoch
        :param loss_val: the val loss of the ith epoch
        :param acc_train: the train accuracy of the ith epoch
        :param acc_val: the val accuracy of the ith epoch
        """

        self.train_loss.append(loss_train)
        self.val_loss.append(loss_val)
        self.train_acc.append(acc_train)
        self.val_acc.append(acc_val)


    def save (self, folder_path):
        """
        This function saves the loss and accuracy history as csv files
        :param folder_path: a string with the base folder path
        """

        path = os.path.join(folder_path, 'history')
        if not os.path.isdir(path):
            os.mkdir(path)

        print ("Saving history CSVs in {}".format(path))

        np.savetxt(os.path.join(path, "train_loss.csv"), np.asarray(self.train_loss), fmt='%.3f', delimiter=',')
        np.savetxt(os.path.join(path, "val_loss.csv"), np.asarray(self.val_loss), fmt='%.3f', delimiter=',')

        np.savetxt(os.path.join(path, "train_acc.csv"), np.asarray(self.train_acc), fmt='%.3f', delimiter=',')
        np.savetxt(os.path.join(path, "val_acc.csv"), np.asarray(self.val_acc), fmt='%.3f', delimiter=',')



    def save_plot (self, folder_path):
        """
        This function saves a plot of the loss and accuracy history
        :param folder_path: a string with the base folder path
        """

        path = os.path.join(folder_path, 'history')
        if not os.path.isdir(path):
            os.mkdir(path)

        epochs = [i + 1 for i in range(len(self.train_loss))]

        print("Saving history plots in {}".format(path))

        plt.plot(epochs, self.train_loss, color='r', linestyle='solid')
        plt.plot(epochs, self.val_loss, color='b', linestyle='solid')
        plt.grid(color='black', linestyle='dotted', linewidth=0.7)
        plt.legend(['Train', 'Validation'], loc='upper right')
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.title("Loss throughout epochs")
        plt.tight_layout()
        plt.savefig(os.path.join(path, "loss_history.png"), dpi=300)

        plt.figure()

        plt.plot(epochs, self.train_acc, color='r', linestyle='solid')
        plt.plot(epochs, self.val_acc, color='b', linestyle='solid')
        plt.grid(color='black', linestyle='dotted', linewidth=0.7)
        plt.legend(['Train', 'Validation'], loc='upper left')
        plt.xlabel("Epoch")
        plt.ylabel("Accuracy")
        plt.title("Accuracy throughout epochs")
        plt.tight_layout()
        plt.savefig(os.path.join(path, "acc_history.png"), dpi=300)

        plt.figure()


def accuracy (output, target, topk=(1,)):
    """
    This function computes the accuracy and top k accuracy for a given predictions and targets
    The difference between this and that one computed by the class is that this one is faster since it is made on
    GPU and during the training/validations phase. There is no need to wait for all batches release their scores
    such as in the classes. However, it's impossible to compute some metrics such as confusion matrix and AUC using
    this function.

    :param output: the predictions outputed by the model
    :param target: the ground truth
    :param topk: the top k accuracy, for example, top 5
    :return: the accuracy an topk accuracy
    """

    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            # correct_k = correct[:k].view(-1).float().sum(0)
            correct_k = correct[:k].reshape(-1).float().sum(0)
            res.append(correct_k.mul_(100.0 / batch_size))

        return res
